{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='stats skills,and prog skills 23450j are imp for5654883214analytics.stats skills and domain knowledge are imp.I like reading and travelling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stats', 'skills', ',', 'and', 'prog', 'skills', '23450j', 'are', 'imp', 'for5654883214analytics.stats', 'skills', 'and', 'domain', 'knowledge', 'are', 'imp.I', 'like', 'reading', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    return re.sub(r'\\d+',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats skills,and prog skills j are imp foranalytics.stats skills and domain knowledge are imp.I like reading and travelling.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_num(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=text.replace(',',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats skillsand prog skills 23450j are imp for5654883214analytics.stats skills and domain knowledge are imp.I like reading and travelling.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=\"hilsee,khjdowfknl3,erg34t55y,,......qw;elgtmf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=s1.replace(\".\",\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hilsee,khjdowfknl3,erg34t55y,,@@@@@@qw;elgtmf'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop words: an,the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d='A paragraph (from the Ancient Greek , parágraphos, \"to write beside\") is a self-contained unit of discourse in writing dealing with a particular point or idea. A paragraph consists of one or more sentences. Though not required by the syntax of any language, paragraphs are usually an expected part of formal writing, used to organize longer prose.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A paragraph (from the Ancient Greek , parágraphos, \"to write beside\") is a self-contained unit of discourse in writing dealing with a particular point or idea. A paragraph consists of one or more sentences. Though not required by the syntax of any language, paragraphs are usually an expected part of formal writing, used to organize longer prose.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_removed(text,lang='english'):\n",
    "    words=nltk.word_tokenize(text)\n",
    "    sw=set(stopwords.words(lang))\n",
    "    sw_removed=[w for w in words if w.lower() not in sw]\n",
    "    return \" \".join(sw_removed)\n",
    "\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-080e43b7e96d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msw_removed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Though not required by the syntax of any language, paragraphs are usually an expected part of formal writing, used to organize longer prose'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-4a3dd4db1c7e>\u001b[0m in \u001b[0;36msw_removed\u001b[1;34m(text, lang)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msw_removed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msw_removed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw_removed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'stopwords'"
     ]
    }
   ],
   "source": [
    "sw_removed('Though not required by the syntax of any language, paragraphs are usually an expected part of formal writing, used to organize longer prose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boya', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boya and the an.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=nltk.pos_tag(nltk.word_tokenize('this is a sample english sent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('english', 'JJ'), ('sent', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming-process of transforming o root words in english\n",
    "bi-two\n",
    "tri-three  \n",
    "\n",
    "ex of root\n",
    "cares,caringly,careful - care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 types of stemmers are available\n",
    "1.Martin porter(porter stemming)\n",
    "2. Lancaster stemming\n",
    "3.Snow ball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer,LancasterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stem(words,type='PorterStemmer',lang='english'):\n",
    "    stemmers=[\"PorterStemmer\",\"LancasterStemmer\",\"SnowballStemmer\"]\n",
    "    if type is False or type not in stemmers:\n",
    "        return words\n",
    "    else:\n",
    "        stem_words=[]\n",
    "        if type == \"PorterStemmer\":\n",
    "            stemmer=PorterStemmer()\n",
    "            for anyword in words:\n",
    "                stem_words.append(stemmer.stem(anyword))\n",
    "                \n",
    "\n",
    "        \n",
    "        if type == \"LancasterStemmer\":\n",
    "            stemmer=LancasterStemmer()\n",
    "            for anyword in words:\n",
    "                stem_words.append(stemmer.stem(anyword))\n",
    "            \n",
    "            \n",
    "        if type == \"SnowballStemmer\":\n",
    "            stemmer=SnowballStemmer(lang)\n",
    "            for anyword in words:\n",
    "                stem_words.append(stemmer.stem(anyword))\n",
    "                \n",
    "        return \"\".join(stem_words)\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "words='caring cares cared caringly carefully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caring cares cared caringly carefully'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stem(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer: carecarecarecaringlicare\n",
      "LancasterStemmer: carcarcarcarcar\n",
      "SnowballStemmer: carecarecarecarecare\n"
     ]
    }
   ],
   "source": [
    "print(\"PorterStemmer:\",word_stem(nltk.word_tokenize(words),\"PorterStemmer\"))\n",
    "print(\"LancasterStemmer:\",word_stem(nltk.word_tokenize(words),\"LancasterStemmer\"))\n",
    "print(\"SnowballStemmer:\",word_stem(nltk.word_tokenize(words),\"SnowballStemmer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization\n",
    "#Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
